{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnijhuis-dnb/open_source_workshop/blob/master/ML_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use machine learning to develop a default model for credit card loans. We will do this by training a classification model which will classify whether or not a customer of a bank will default on their credit card loan. We will also inspect whether or not our model identifies the most logical features to base its predictions on. For the data we will be using a public data set of credit card data from a Taiwanese bank. "
      ],
      "metadata": {
        "id": "PgBsE2vJba75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to install some packages"
      ],
      "metadata": {
        "id": "NSYhKQGMNmRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "!pip install --upgrade xlrd"
      ],
      "metadata": {
        "id": "WdVpp-JcpQv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will download the data"
      ],
      "metadata": {
        "id": "7hXqf_vONrgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
      ],
      "metadata": {
        "id": "9OcMcaq-qYQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will import the packages we are going to use"
      ],
      "metadata": {
        "id": "dk0RZYkuNumY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import minmax_scale"
      ],
      "metadata": {
        "id": "lPTZok08qUXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set the max display of pandas to show more columns and rows"
      ],
      "metadata": {
        "id": "WFaHuBN5N-gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "xRhSz4TQrmFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We read in the data"
      ],
      "metadata": {
        "id": "rMnZ4emrOMXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/default of credit card clients.xls', header=1, index_col=0).rename(columns={'default payment next month':'DEFAULTS'})"
      ],
      "metadata": {
        "id": "lpNOpXuyqapH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIMIT_BAL: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
        "\n",
        "SEX: Gender (1 = male; 2 = female).\n",
        "\n",
        "EDUCATION: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "\n",
        "MARRIAGE: Marital status (1 = married; 2 = single; 3 = others).\n",
        "\n",
        "AGE: Age (year).\n",
        "\n",
        "PAY_0 - PAY_6: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: PAY_0 = the repayment status in September, 2005; PAY_1 = the repayment status in August, 2005; . . .;PAY_6 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
        "\n",
        "BILL_AMT1-BILL_AMT6: Amount of bill statement (NT dollar). BILL_AMT1 = amount of bill statement in September, 2005; BILL_AMT2 = amount of bill statement in August, 2005; . . .; BILL_AMT6 = amount of bill statement in April, 2005.\n",
        "\n",
        "PAY_AMT1-PAY_AMT6: Amount of previous payment (NT dollar). PAY_AMT1 = amount paid in September, 2005; PAY_AMT2 = amount paid in August, 2005; . . .;PAY_AMT6 = amount paid in April, 2005.\n",
        "\n",
        "DEFAULTS: Default payment next month (1 = defaulted; 0 = not defaulted)"
      ],
      "metadata": {
        "id": "9FPpLQNnuLF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look at the data"
      ],
      "metadata": {
        "id": "hNd7PJ1_QaBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "xKL5ydDUQb1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "npdw3Z2lOPX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to do some cleaning of the data"
      ],
      "metadata": {
        "id": "uxgB3-WOPoXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to see all the values in the EDUCATION column and how often they occur"
      ],
      "metadata": {
        "id": "XYZuIupqPvuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "R_C8_CcWPtCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't know what the education number 0, 5 and 6 mean, so we will have to do something with it, we can drop these rows"
      ],
      "metadata": {
        "id": "CuXQ0XgfQBLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[~df['EDUCATION'].isin([0,5,6])]"
      ],
      "metadata": {
        "id": "jEEMrekOQPnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or convert the numbers we don't know to the other category"
      ],
      "metadata": {
        "id": "qimFQyu3QegD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['EDUCATION'].isin([0,5,6]),'EDUCATION'] = 4"
      ],
      "metadata": {
        "id": "LH-kV7ZYPrOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Determine if other data cleaning steps need to be taken and clean the data further if**"
      ],
      "metadata": {
        "id": "nMuHxLjwQol4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1NL-lGv_QsKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is the conversion of categorical variables to numerical values"
      ],
      "metadata": {
        "id": "t67qjPobRZDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first do the one-hot-encoding\n",
        "education_dummies = pd.get_dummies(df['EDUCATION'])\n",
        "\n",
        "# rename the columns to more understandable values\n",
        "education_dummies = education_dummies.rename(columns={1:'GRADUTE_SCHOOL', 2:'UNIVERSITY', 3:'HIGH_SCHOOL',  4:'OTHERS_EDUCATION'})\n",
        "\n",
        "# drop the old EDUCATION column\n",
        "df = df.drop(columns='EDUCATION')\n",
        "\n",
        "# combine the education dummies with the original data \n",
        "df = pd.merge(df, education_dummies, left_index=True, right_index=True)"
      ],
      "metadata": {
        "id": "DbHknN2FRltZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Determine if other columns have categorical variables and encode these columns**"
      ],
      "metadata": {
        "id": "K3y3-_HKRkaB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HmzYrWpzSuWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to normalise the data"
      ],
      "metadata": {
        "id": "7d9abe8nSwWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply min-max scaling to the age column"
      ],
      "metadata": {
        "id": "1HMyvSs9UNLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['AGE'] = minmax_scale(df['AGE'])"
      ],
      "metadata": {
        "id": "k1dsPMzkTWZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Determine if other columns should be normalised and normalise them, do take into account that certain columns are related to eachother**"
      ],
      "metadata": {
        "id": "516eS8T2UTdn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PlIAihNSUoo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "pCY0z_wgUo-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step is to make a test-train split in the data"
      ],
      "metadata": {
        "id": "V5q5QxXlU5M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = df.drop(columns='DEFAULTS')\n",
        "y_data = df['DEFAULTS']"
      ],
      "metadata": {
        "id": "LE2fxYH_actX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(x_data, y_data, random_state=1)"
      ],
      "metadata": {
        "id": "XLK_kdNSaN2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check whether the data is balanced enough to make a prediction and rebalance if needed**"
      ],
      "metadata": {
        "id": "Lf7aEOqWaudx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgSv0GOsbX0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define the actual model. We are going to use a random forest model. In this model we have a few parameters we can adjust to impact the performance of the model. Below the parameters are explained in a bit more detail.\n",
        "\n",
        "* ***max_depth*** : The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than\n",
        "min_samples_split samples.\n",
        "* ***min_samples_split*** : The minimum number of samples required to split an internal node:\n",
        "* ***min_samples_leaf*** : The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at\n",
        "least min_samples_leaf training samples in each of the left and\n",
        "right branches.  \n",
        "* ***max_leaf_nodes*** : Grow trees with max_leaf_nodes in best-first fashion.\n",
        "Best nodes are defined as relative reduction in impurity.\n",
        "If None then unlimited number of leaf nodes.\n",
        "* ***bootstrap*** : Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
        "* ***oob_score*** : Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True."
      ],
      "metadata": {
        "id": "nwBsCnVkcpox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(random_state = 2, \n",
        "                               max_depth = 500, \n",
        "                               min_samples_split = 2, \n",
        "                               min_samples_leaf = 1, \n",
        "                               max_leaf_nodes = 10000, \n",
        "                               bootstrap = False,\n",
        "                               oob_score = False)"
      ],
      "metadata": {
        "id": "15JdXtgfcp-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the model is defined we can train the model"
      ],
      "metadata": {
        "id": "RFgdMHOXdSVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.fit(train_x.values, train_y.values)"
      ],
      "metadata": {
        "id": "BWj85Y06dSqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the model is trained, we can try and make predictions based on the test data"
      ],
      "metadata": {
        "id": "v1c-qn1odYAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predict = model.predict(test_x.values)"
      ],
      "metadata": {
        "id": "0-kEwSFuOjrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " With the predictions we can evaluate the performance of the modelx, by making a confussion table"
      ],
      "metadata": {
        "id": "S3kkmFShOvjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_table = confusion_matrix(test_y, test_predict)\n",
        "confusion_table"
      ],
      "metadata": {
        "id": "RZXboCF5PipA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Determine the accuracy, precision and recall of the results**"
      ],
      "metadata": {
        "id": "CwBlCRMWdg6p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_haeHIlPdryp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you improve the quality of the forecast of the model by adjusting the parameters**"
      ],
      "metadata": {
        "id": "hKKBYOHVdsFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the prediction"
      ],
      "metadata": {
        "id": "Kj4P7A7KbYIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further evaluate the prediction we will be using the shap package to calculate the shapley values and see which columns are most important to our predictions"
      ],
      "metadata": {
        "id": "QNK6qxARSS2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we start looking at a specific prediction, we are going to predict the class of the 54th row of the test data"
      ],
      "metadata": {
        "id": "X9qKw0uxSkVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_number = 54\n",
        "data_for_prediction = test_x.iloc[row_number]  \n",
        "model.predict_proba(data_for_prediction.values.reshape(1, -1))"
      ],
      "metadata": {
        "id": "mLPB0bhXSnk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can caluculate the shapley values from the data"
      ],
      "metadata": {
        "id": "LuUBZvO8S6Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(data_for_prediction)\n",
        "shap_values"
      ],
      "metadata": {
        "id": "QaHJnqcvSOtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These don't say much on their own, let's plot them"
      ],
      "metadata": {
        "id": "vDH45gHgTbLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[1], test_x.loc[test_x.index[row_number]])"
      ],
      "metadata": {
        "id": "FuyzVsQ_TiEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you find some interesting prediction reasons?**"
      ],
      "metadata": {
        "id": "VwRD4mo6Tm6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at all the rows at the same time (this does take some time to calculate depending on the size of your model, so we only use a subset of the data)"
      ],
      "metadata": {
        "id": "b6x3BFVUTtLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "explainer = shap.TreeExplainer(model)\n",
        "subset = test_x.iloc[:1000,:]\n",
        "shap_values = explainer.shap_values(subset)\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[1], subset)"
      ],
      "metadata": {
        "id": "zksujX08Tyz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or in a simpler overview"
      ],
      "metadata": {
        "id": "4fRS5fklT6Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values[1], subset)"
      ],
      "metadata": {
        "id": "KQyWfT3ET5h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What can you say about the factors which are most important for the prediction of the class?**"
      ],
      "metadata": {
        "id": "gBHRVq_mU8fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also look at the interactions between two columns"
      ],
      "metadata": {
        "id": "Y-BtHnSFVD45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.dependence_plot('LIMIT_BAL', shap_values[1], subset, interaction_index='PAY_0', x_jitter=1, dot_size=20)"
      ],
      "metadata": {
        "id": "WIV8GevoVMjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you find some interesting interactions**"
      ],
      "metadata": {
        "id": "Alf1zaCbXRhl"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Data Table Display",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}